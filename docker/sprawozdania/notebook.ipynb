{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18d719c7-6d9e-42b3-abaf-f0e4cad11db4",
   "metadata": {},
   "source": [
    "# How to run commands on hadoop using jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b8abc-9233-40a5-80c7-8ff991a48b64",
   "metadata": {},
   "source": [
    "## Import utility functions\n",
    "run_in_master runs provided command on namenode. Use it to start map-reduce, pig or spark application. \n",
    "\n",
    "run_in_hive runs provided command on hive-server. Use it to start hive application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ae2f6a-fe7c-4543-aaf1-e347de4b7d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import run_in_master, run_in_hive, print_hdfs_output, hdfs_upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b9e7712-ab2d-4908-bb5b-87cc85b0d592",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['test\\n'], [])\n",
      "(['test\\n'], [])\n"
     ]
    }
   ],
   "source": [
    "print(run_in_master(\"echo 'test'\"))\n",
    "print(run_in_hive(\"echo 'test'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c403ac5-3250-470a-b203-483a09dd983e",
   "metadata": {},
   "source": [
    "# Map reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4254bb1-7424-4935-9e61-5a1d8358d911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Number of Maps  = 2\\n',\n",
       "  'Samples per Map = 5\\n',\n",
       "  'Wrote input for Map #0\\n',\n",
       "  'Wrote input for Map #1\\n',\n",
       "  'Starting Job\\n',\n",
       "  'Job Finished in 13.097 seconds\\n',\n",
       "  'Estimated value of Pi is 3.60000000000000000000\\n'],\n",
       " ['2023-06-19 22:27:47,595 INFO client.DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:27:47,662 INFO client.AHSProxy: Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:27:47,747 INFO mapreduce.JobResourceUploader: Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1687213661089_0001\\n',\n",
       "  '2023-06-19 22:27:47,820 INFO input.FileInputFormat: Total input files to process : 2\\n',\n",
       "  '2023-06-19 22:27:47,865 INFO mapreduce.JobSubmitter: number of splits:2\\n',\n",
       "  '2023-06-19 22:27:47,938 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1687213661089_0001\\n',\n",
       "  '2023-06-19 22:27:47,938 INFO mapreduce.JobSubmitter: Executing with tokens: []\\n',\n",
       "  '2023-06-19 22:27:48,037 INFO conf.Configuration: resource-types.xml not found\\n',\n",
       "  \"2023-06-19 22:27:48,038 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\\n\",\n",
       "  '2023-06-19 22:27:48,358 INFO impl.YarnClientImpl: Submitted application application_1687213661089_0001\\n',\n",
       "  '2023-06-19 22:27:48,382 INFO mapreduce.Job: The url to track the job: http://resourcemanager:8088/proxy/application_1687213661089_0001/\\n',\n",
       "  '2023-06-19 22:27:48,382 INFO mapreduce.Job: Running job: job_1687213661089_0001\\n',\n",
       "  '2023-06-19 22:27:53,485 INFO mapreduce.Job: Job job_1687213661089_0001 running in uber mode : false\\n',\n",
       "  '2023-06-19 22:27:53,486 INFO mapreduce.Job:  map 0% reduce 0%\\n',\n",
       "  '2023-06-19 22:27:56,512 INFO mapreduce.Job:  map 50% reduce 0%\\n',\n",
       "  '2023-06-19 22:27:57,514 INFO mapreduce.Job:  map 100% reduce 0%\\n',\n",
       "  '2023-06-19 22:28:00,522 INFO mapreduce.Job:  map 100% reduce 100%\\n',\n",
       "  '2023-06-19 22:28:00,526 INFO mapreduce.Job: Job job_1687213661089_0001 completed successfully\\n',\n",
       "  '2023-06-19 22:28:00,576 INFO mapreduce.Job: Counters: 54\\n',\n",
       "  '\\tFile System Counters\\n',\n",
       "  '\\t\\tFILE: Number of bytes read=40\\n',\n",
       "  '\\t\\tFILE: Number of bytes written=829864\\n',\n",
       "  '\\t\\tFILE: Number of read operations=0\\n',\n",
       "  '\\t\\tFILE: Number of large read operations=0\\n',\n",
       "  '\\t\\tFILE: Number of write operations=0\\n',\n",
       "  '\\t\\tHDFS: Number of bytes read=526\\n',\n",
       "  '\\t\\tHDFS: Number of bytes written=215\\n',\n",
       "  '\\t\\tHDFS: Number of read operations=13\\n',\n",
       "  '\\t\\tHDFS: Number of large read operations=0\\n',\n",
       "  '\\t\\tHDFS: Number of write operations=3\\n',\n",
       "  '\\t\\tHDFS: Number of bytes read erasure-coded=0\\n',\n",
       "  '\\tJob Counters \\n',\n",
       "  '\\t\\tLaunched map tasks=2\\n',\n",
       "  '\\t\\tLaunched reduce tasks=1\\n',\n",
       "  '\\t\\tRack-local map tasks=2\\n',\n",
       "  '\\t\\tTotal time spent by all maps in occupied slots (ms)=11904\\n',\n",
       "  '\\t\\tTotal time spent by all reduces in occupied slots (ms)=4968\\n',\n",
       "  '\\t\\tTotal time spent by all map tasks (ms)=2976\\n',\n",
       "  '\\t\\tTotal time spent by all reduce tasks (ms)=1242\\n',\n",
       "  '\\t\\tTotal vcore-milliseconds taken by all map tasks=2976\\n',\n",
       "  '\\t\\tTotal vcore-milliseconds taken by all reduce tasks=1242\\n',\n",
       "  '\\t\\tTotal megabyte-milliseconds taken by all map tasks=12189696\\n',\n",
       "  '\\t\\tTotal megabyte-milliseconds taken by all reduce tasks=5087232\\n',\n",
       "  '\\tMap-Reduce Framework\\n',\n",
       "  '\\t\\tMap input records=2\\n',\n",
       "  '\\t\\tMap output records=4\\n',\n",
       "  '\\t\\tMap output bytes=36\\n',\n",
       "  '\\t\\tMap output materialized bytes=49\\n',\n",
       "  '\\t\\tInput split bytes=290\\n',\n",
       "  '\\t\\tCombine input records=0\\n',\n",
       "  '\\t\\tCombine output records=0\\n',\n",
       "  '\\t\\tReduce input groups=2\\n',\n",
       "  '\\t\\tReduce shuffle bytes=49\\n',\n",
       "  '\\t\\tReduce input records=4\\n',\n",
       "  '\\t\\tReduce output records=0\\n',\n",
       "  '\\t\\tSpilled Records=8\\n',\n",
       "  '\\t\\tShuffled Maps =2\\n',\n",
       "  '\\t\\tFailed Shuffles=0\\n',\n",
       "  '\\t\\tMerged Map outputs=2\\n',\n",
       "  '\\t\\tGC time elapsed (ms)=85\\n',\n",
       "  '\\t\\tCPU time spent (ms)=640\\n',\n",
       "  '\\t\\tPhysical memory (bytes) snapshot=1017253888\\n',\n",
       "  '\\t\\tVirtual memory (bytes) snapshot=15117545472\\n',\n",
       "  '\\t\\tTotal committed heap usage (bytes)=1130889216\\n',\n",
       "  '\\t\\tPeak Map Physical memory (bytes)=374374400\\n',\n",
       "  '\\t\\tPeak Map Virtual memory (bytes)=5040160768\\n',\n",
       "  '\\t\\tPeak Reduce Physical memory (bytes)=271228928\\n',\n",
       "  '\\t\\tPeak Reduce Virtual memory (bytes)=5042053120\\n',\n",
       "  '\\tShuffle Errors\\n',\n",
       "  '\\t\\tBAD_ID=0\\n',\n",
       "  '\\t\\tCONNECTION=0\\n',\n",
       "  '\\t\\tIO_ERROR=0\\n',\n",
       "  '\\t\\tWRONG_LENGTH=0\\n',\n",
       "  '\\t\\tWRONG_MAP=0\\n',\n",
       "  '\\t\\tWRONG_REDUCE=0\\n',\n",
       "  '\\tFile Input Format Counters \\n',\n",
       "  '\\t\\tBytes Read=236\\n',\n",
       "  '\\tFile Output Format Counters \\n',\n",
       "  '\\t\\tBytes Written=97\\n'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_master(\"yarn jar /opt/hadoop-3.3.1/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar pi 2 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aef1e7-3d5a-48df-84f1-3b56140d5a94",
   "metadata": {},
   "source": [
    "# Pig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c63ab2-5e30-4282-8432-859149fd4667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -put /data/master_volume/examples/pig.pig /examples\n",
      "exit code []\n",
      "[]\n",
      "hdfs dfs -put /data/master_volume/examples/data.jsonl /examples\n",
      "exit code []\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "hdfs_upload(\"examples/pig.pig\")\n",
    "hdfs_upload(\"examples/data.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0917877a-508b-403f-a64b-0b296fedd65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['(Micha≈Ç,24)\\n'],\n",
       " ['2023-06-19 22:28:06,352 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\\n',\n",
       "  '2023-06-19 22:28:06,352 INFO pig.ExecTypeProvider: Trying ExecType : MAPREDUCE\\n',\n",
       "  '2023-06-19 22:28:06,352 INFO pig.ExecTypeProvider: Picked MAPREDUCE as the ExecType\\n',\n",
       "  '2023-06-19 22:28:06,376 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\\n',\n",
       "  '2023-06-19 22:28:06,376 [main] INFO  org.apache.pig.Main - Logging error messages to: /app/pig_1687213686372.log\\n',\n",
       "  '2023-06-19 22:28:06,512 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\\n',\n",
       "  '2023-06-19 22:28:06,532 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\\n',\n",
       "  '2023-06-19 22:28:06,532 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: hdfs://namenode:9000\\n',\n",
       "  '2023-06-19 22:28:06,751 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-pig.pig-bafa3559-7bad-45ff-9c25-35059bbce0b1\\n',\n",
       "  '2023-06-19 22:28:06,843 [main] INFO  org.apache.hadoop.yarn.client.api.impl.TimelineClientImpl - Timeline service address: historyserver:8188\\n',\n",
       "  '2023-06-19 22:28:06,979 [main] INFO  org.apache.pig.backend.hadoop.PigATSClient - Created ATS Hook\\n',\n",
       "  '2023-06-19 22:28:06,988 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,174 [main] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (PS Old Gen) of size 699400192 to monitor. collectionUsageThreshold = 489580128, usageThreshold = 489580128\\n',\n",
       "  '2023-06-19 22:28:07,232 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,240 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,291 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,298 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: UNKNOWN\\n',\n",
       "  '2023-06-19 22:28:07,307 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,311 [main] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\\n',\n",
       "  '2023-06-19 22:28:07,324 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NestedLimitOptimizer, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}\\n',\n",
       "  '2023-06-19 22:28:07,362 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\\n',\n",
       "  '2023-06-19 22:28:07,372 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 1\\n',\n",
       "  '2023-06-19 22:28:07,372 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 1\\n',\n",
       "  '2023-06-19 22:28:07,381 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,406 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:28:07,489 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:28:07,524 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\\n',\n",
       "  '2023-06-19 22:28:07,527 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\\n',\n",
       "  '2023-06-19 22:28:07,527 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\\n',\n",
       "  '2023-06-19 22:28:07,528 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\\n',\n",
       "  '2023-06-19 22:28:07,528 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - This job cannot be converted run in-process\\n',\n",
       "  '2023-06-19 22:28:07,532 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.submit.replication is deprecated. Instead, use mapreduce.client.submit.file.replication\\n',\n",
       "  '2023-06-19 22:28:07,646 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/pig-0.17.0-core-h2.jar to DistributedCache through /tmp/temp1922904926/tmp81945952/pig-0.17.0-core-h2.jar\\n',\n",
       "  '2023-06-19 22:28:07,668 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/opt/hadoop-3.3.1/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar to DistributedCache through /tmp/temp1922904926/tmp1101839574/jackson-core-asl-1.9.13.jar\\n',\n",
       "  '2023-06-19 22:28:07,687 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/lib/automaton-1.11-8.jar to DistributedCache through /tmp/temp1922904926/tmp-669566523/automaton-1.11-8.jar\\n',\n",
       "  '2023-06-19 22:28:07,707 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/lib/antlr-runtime-3.4.jar to DistributedCache through /tmp/temp1922904926/tmp1201580005/antlr-runtime-3.4.jar\\n',\n",
       "  '2023-06-19 22:28:07,728 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Added jar file:/usr/local/pig/lib/joda-time-2.9.3.jar to DistributedCache through /tmp/temp1922904926/tmp-1530290103/joda-time-2.9.3.jar\\n',\n",
       "  '2023-06-19 22:28:07,734 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\\n',\n",
       "  '2023-06-19 22:28:07,757 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\\n',\n",
       "  '2023-06-19 22:28:07,760 [JobControl] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:28:07,760 [JobControl] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:28:07,766 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\\n',\n",
       "  '2023-06-19 22:28:07,767 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,806 [JobControl] INFO  org.apache.hadoop.mapreduce.JobResourceUploader - Disabling Erasure Coding for path: /tmp/hadoop-yarn/staging/root/.staging/job_1687213661089_0002\\n',\n",
       "  '2023-06-19 22:28:07,812 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\\n',\n",
       "  '2023-06-19 22:28:07,842 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\\n',\n",
       "  '2023-06-19 22:28:07,853 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\\n',\n",
       "  '2023-06-19 22:28:07,893 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\\n',\n",
       "  '2023-06-19 22:28:07,909 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:07,953 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_1687213661089_0002\\n',\n",
       "  '2023-06-19 22:28:07,953 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\\n',\n",
       "  '2023-06-19 22:28:08,008 [JobControl] INFO  org.apache.hadoop.mapred.YARNRunner - Job jar is not present. Not adding any jar to the list of resources.\\n',\n",
       "  '2023-06-19 22:28:08,033 [JobControl] INFO  org.apache.hadoop.conf.Configuration - resource-types.xml not found\\n',\n",
       "  \"2023-06-19 22:28:08,033 [JobControl] INFO  org.apache.hadoop.yarn.util.resource.ResourceUtils - Unable to find 'resource-types.xml'.\\n\",\n",
       "  '2023-06-19 22:28:08,267 [JobControl] INFO  org.apache.hadoop.yarn.client.api.impl.YarnClientImpl - Submitted application application_1687213661089_0002\\n',\n",
       "  '2023-06-19 22:28:08,286 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://resourcemanager:8088/proxy/application_1687213661089_0002/\\n',\n",
       "  '2023-06-19 22:28:08,286 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_1687213661089_0002\\n',\n",
       "  '2023-06-19 22:28:08,286 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases lines\\n',\n",
       "  '2023-06-19 22:28:08,286 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: lines[1,8] C:  R: \\n',\n",
       "  '2023-06-19 22:28:08,291 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\\n',\n",
       "  '2023-06-19 22:28:08,291 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1687213661089_0002]\\n',\n",
       "  '2023-06-19 22:28:15,318 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\\n',\n",
       "  '2023-06-19 22:28:15,318 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_1687213661089_0002]\\n',\n",
       "  '2023-06-19 22:28:18,324 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:28:18,325 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:28:18,328 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:18,431 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:18,534 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:18,636 [main] WARN  org.apache.pig.tools.pigstats.mapreduce.MRJobStats - Failed to get map task report\\n',\n",
       "  'java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:345)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:872)\\n',\n",
       "  '\\tat org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:215)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getTaskReports(MRJobStats.java:528)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addMapReduceStatistics(MRJobStats.java:355)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:232)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:164)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:379)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:290)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.launchPlan(PigServer.java:1475)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1460)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.storeEx(PigServer.java:1119)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.store(PigServer.java:1082)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.openIterator(PigServer.java:995)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:782)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:383)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\\n',\n",
       "  '\\tat org.apache.pig.Main.run(Main.java:630)\\n',\n",
       "  '\\tat org.apache.pig.Main.main(Main.java:175)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n',\n",
       "  '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n',\n",
       "  '\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\\n',\n",
       "  'Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)\\n',\n",
       "  '2023-06-19 22:28:18,639 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:28:18,639 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:28:18,642 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:18,744 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:18,847 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:18,949 [main] WARN  org.apache.pig.tools.pigstats.mapreduce.MRJobStats - Failed to get reduce task report\\n',\n",
       "  'java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:345)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:872)\\n',\n",
       "  '\\tat org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:215)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getTaskReports(MRJobStats.java:528)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addMapReduceStatistics(MRJobStats.java:361)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:232)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:164)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:379)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:290)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.launchPlan(PigServer.java:1475)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1460)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.storeEx(PigServer.java:1119)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.store(PigServer.java:1082)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.openIterator(PigServer.java:995)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:782)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:383)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\\n',\n",
       "  '\\tat org.apache.pig.Main.run(Main.java:630)\\n',\n",
       "  '\\tat org.apache.pig.Main.main(Main.java:175)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n',\n",
       "  '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n',\n",
       "  '\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\\n',\n",
       "  'Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)\\n',\n",
       "  '2023-06-19 22:28:18,949 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\\n',\n",
       "  '2023-06-19 22:28:18,949 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\\n',\n",
       "  '2023-06-19 22:28:18,951 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:28:18,951 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:28:18,954 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,058 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,161 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,263 [main] WARN  org.apache.pig.tools.pigstats.mapreduce.MRJobStats - Unable to get job counters\\n',\n",
       "  'java.io.IOException: java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getCounters(MRJobStats.java:548)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.addCounters(MRJobStats.java:287)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.addSuccessJobStats(MRPigStatsUtil.java:234)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil.accumulateStats(MRPigStatsUtil.java:164)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:379)\\n',\n",
       "  '\\tat org.apache.pig.backend.hadoop.executionengine.HExecutionEngine.launchPig(HExecutionEngine.java:290)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.launchPlan(PigServer.java:1475)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1460)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.storeEx(PigServer.java:1119)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.store(PigServer.java:1082)\\n',\n",
       "  '\\tat org.apache.pig.PigServer.openIterator(PigServer.java:995)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:782)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:383)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:230)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:205)\\n',\n",
       "  '\\tat org.apache.pig.tools.grunt.Grunt.exec(Grunt.java:81)\\n',\n",
       "  '\\tat org.apache.pig.Main.run(Main.java:630)\\n',\n",
       "  '\\tat org.apache.pig.Main.main(Main.java:175)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n',\n",
       "  '\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n',\n",
       "  '\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n',\n",
       "  '\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.run(RunJar.java:323)\\n',\n",
       "  '\\tat org.apache.hadoop.util.RunJar.main(RunJar.java:236)\\n',\n",
       "  'Caused by: java.io.IOException: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:345)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:430)\\n',\n",
       "  '\\tat org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:872)\\n',\n",
       "  '\\tat org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:215)\\n',\n",
       "  '\\tat org.apache.pig.tools.pigstats.mapreduce.MRJobStats.getCounters(MRJobStats.java:542)\\n',\n",
       "  '\\t... 23 more\\n',\n",
       "  'Caused by: org.apache.hadoop.ipc.RpcException: RPC response exceeds maximum data length\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1906)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1202)\\n',\n",
       "  '\\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1098)\\n',\n",
       "  '2023-06-19 22:28:19,265 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\\n',\n",
       "  '2023-06-19 22:28:19,267 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: \\n',\n",
       "  '\\n',\n",
       "  'HadoopVersion\\tPigVersion\\tUserId\\tStartedAt\\tFinishedAt\\tFeatures\\n',\n",
       "  '3.3.1\\t0.17.0\\troot\\t2023-06-19 22:28:07\\t2023-06-19 22:28:19\\tUNKNOWN\\n',\n",
       "  '\\n',\n",
       "  'Success!\\n',\n",
       "  '\\n',\n",
       "  'Job Stats (time in seconds):\\n',\n",
       "  'JobId\\tMaps\\tReduces\\tMaxMapTime\\tMinMapTime\\tAvgMapTime\\tMedianMapTime\\tMaxReduceTime\\tMinReduceTime\\tAvgReduceTime\\tMedianReducetime\\tAlias\\tFeature\\tOutputs\\n',\n",
       "  'job_1687213661089_0002\\t1\\t0\\tn/a\\tn/a\\tn/a\\tn/a\\t0\\t0\\t0\\t0\\tlines\\tMAP_ONLY\\thdfs://namenode:9000/tmp/temp1922904926/tmp-1326203289,\\n',\n",
       "  '\\n',\n",
       "  'Input(s):\\n',\n",
       "  'Successfully read 0 records from: \"/examples/data.jsonl\"\\n',\n",
       "  '\\n',\n",
       "  'Output(s):\\n',\n",
       "  'Successfully stored 0 records in: \"hdfs://namenode:9000/tmp/temp1922904926/tmp-1326203289\"\\n',\n",
       "  '\\n',\n",
       "  'Counters:\\n',\n",
       "  'Total records written : 0\\n',\n",
       "  'Total bytes written : 0\\n',\n",
       "  'Spillable Memory Manager spill count : 0\\n',\n",
       "  'Total bags proactively spilled: 0\\n',\n",
       "  'Total records proactively spilled: 0\\n',\n",
       "  '\\n',\n",
       "  'Job DAG:\\n',\n",
       "  'job_1687213661089_0002\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '2023-06-19 22:28:19,269 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:28:19,269 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:28:19,272 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,375 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,478 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,579 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Unable to get job related diagnostics\\n',\n",
       "  '2023-06-19 22:28:19,581 [main] INFO  org.apache.hadoop.yarn.client.DefaultNoHARMFailoverProxyProvider - Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '2023-06-19 22:28:19,581 [main] INFO  org.apache.hadoop.yarn.client.AHSProxy - Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '2023-06-19 22:28:19,583 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,686 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,788 [main] INFO  org.apache.hadoop.mapred.ClientServiceDelegate - Application state is completed. FinalApplicationStatus=SUCCEEDED. Redirecting to job history server\\n',\n",
       "  '2023-06-19 22:28:19,890 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Unable to retrieve job to compute warning aggregation.\\n',\n",
       "  '2023-06-19 22:28:19,890 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\\n',\n",
       "  '2023-06-19 22:28:19,892 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:19,892 [main] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\\n',\n",
       "  '2023-06-19 22:28:19,899 [main] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\\n',\n",
       "  '2023-06-19 22:28:19,899 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\\n',\n",
       "  '2023-06-19 22:28:19,934 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - yarn.resourcemanager.system-metrics-publisher.enabled is deprecated. Instead, use yarn.system-metrics-publisher.enabled\\n',\n",
       "  '2023-06-19 22:28:19,944 [main] INFO  org.apache.pig.Main - Pig script completed in 13 seconds and 610 milliseconds (13610 ms)\\n'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_master(\"pig -x mapreduce /data/master_volume/examples/pig.pig\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db7d2f7-4616-42c0-8408-db4cd60fe72c",
   "metadata": {},
   "source": [
    "# Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a3e5d0-1a04-425e-8ba2-af502d8af2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs dfs -put /data/master_volume/examples/employee.csv /examples\n",
      "exit code []\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "hdfs_upload(\"examples/employee.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c32bce82-69af-4529-8efa-d33ce61e78d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  'Hive Session ID = a2330ac2-ec8a-4772-83a9-db4085f23bf0\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'Hive Session ID = d339b115-9a29-483e-bf81-98eaa4b5cfa4\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.483 seconds\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.027 seconds\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.291 seconds\\n'])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/examples/employee_table.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56b6969b-6ade-4c41-8a39-a92c58ec1e54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  'Hive Session ID = c71a58b7-1c74-4d88-834e-fb06fa189138\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'Hive Session ID = 2bf7c717-364c-42c3-bbb8-50d9428c7b98\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.398 seconds\\n',\n",
       "  'Loading data to table testdb.employee\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.48 seconds\\n'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/examples/employee_load.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4395759b-d00e-4190-bf5e-da5162232354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['SLF4J: Class path contains multiple SLF4J bindings.\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hive/lib/log4j-slf4j-impl-2.10.0.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: Found binding in [jar:file:/opt/hadoop-3.3.1/share/hadoop/common/lib/slf4j-log4j12-1.7.30.jar!/org/slf4j/impl/StaticLoggerBinder.class]\\n',\n",
       "  'SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\\n',\n",
       "  'SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\\n',\n",
       "  'Hive Session ID = 45219d87-b12f-40ae-8678-73b9848d64b3\\n',\n",
       "  '\\n',\n",
       "  'Logging initialized using configuration in file:/opt/hive/conf/hive-log4j2.properties Async: true\\n',\n",
       "  'Hive Session ID = bf781ccc-0084-4a2d-a6a3-e1ffeeae9093\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 0.395 seconds\\n',\n",
       "  'Query ID = root_20230619222833_d2f7a4ae-4e7b-4c0b-ae8b-186d28c2daf8\\n',\n",
       "  'Total jobs = 1\\n',\n",
       "  'Launching Job 1 out of 1\\n',\n",
       "  'Number of reduce tasks not specified. Estimated from input data size: 1\\n',\n",
       "  'In order to change the average load for a reducer (in bytes):\\n',\n",
       "  '  set hive.exec.reducers.bytes.per.reducer=<number>\\n',\n",
       "  'In order to limit the maximum number of reducers:\\n',\n",
       "  '  set hive.exec.reducers.max=<number>\\n',\n",
       "  'In order to set a constant number of reducers:\\n',\n",
       "  '  set mapreduce.job.reduces=<number>\\n',\n",
       "  'Starting Job = job_1687213661089_0003, Tracking URL = http://resourcemanager:8088/proxy/application_1687213661089_0003/\\n',\n",
       "  'Kill Command = /opt/hadoop-3.3.1/bin/mapred job  -kill job_1687213661089_0003\\n',\n",
       "  'Hadoop job information for Stage-1: number of mappers: 1; number of reducers: 1\\n',\n",
       "  '2023-06-19 22:28:38,452 Stage-1 map = 0%,  reduce = 0%\\n',\n",
       "  '2023-06-19 22:28:42,538 Stage-1 map = 100%,  reduce = 0%, Cumulative CPU 1.45 sec\\n',\n",
       "  '2023-06-19 22:28:46,596 Stage-1 map = 100%,  reduce = 100%, Cumulative CPU 2.77 sec\\n',\n",
       "  'MapReduce Total cumulative CPU time: 2 seconds 770 msec\\n',\n",
       "  'Ended Job = job_1687213661089_0003\\n',\n",
       "  'Moving data to directory hdfs://namenode:9000/user/hive/warehouse/results\\n',\n",
       "  'MapReduce Jobs Launched: \\n',\n",
       "  'Stage-Stage-1: Map: 1  Reduce: 1   Cumulative CPU: 2.77 sec   HDFS Read: 66935 HDFS Write: 11665 SUCCESS\\n',\n",
       "  'Total MapReduce CPU Time Spent: 2 seconds 770 msec\\n',\n",
       "  'OK\\n',\n",
       "  'Time taken: 15.729 seconds\\n'])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_hive(\"hive -f /data/master_volume/examples/test_group.hql\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92266f81-f14e-49dd-8dd0-74451a7b004e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88249\u000133\n",
      "\n",
      "14555\u0001152\n",
      "\n",
      "13918\u0001264\n",
      "\n",
      "22224\u0001443\n",
      "\n",
      "83793\u0001498\n",
      "\n",
      "77255\u0001506\n",
      "\n",
      "92432\u0001520\n",
      "\n",
      "23576\u0001545\n",
      "\n",
      "37365\u0001571\n",
      "\n",
      "26259\u0001585\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(\"/user/hive/warehouse/results/000000_0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841b42e7-8782-4459-8c7e-1be51b1787e6",
   "metadata": {},
   "source": [
    "# Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06fae32b-eddd-4157-ac81-c0255bb1dbed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([],\n",
       " ['23/06/19 22:28:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\\n',\n",
       "  '23/06/19 22:28:51 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at resourcemanager/172.19.0.2:8032\\n',\n",
       "  '23/06/19 22:28:51 INFO AHSProxy: Connecting to Application History server at historyserver/172.19.0.9:10200\\n',\n",
       "  '23/06/19 22:28:51 INFO Configuration: resource-types.xml not found\\n',\n",
       "  \"23/06/19 22:28:51 INFO ResourceUtils: Unable to find 'resource-types.xml'.\\n\",\n",
       "  '23/06/19 22:28:51 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (4096 MB per container)\\n',\n",
       "  '23/06/19 22:28:51 INFO Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead\\n',\n",
       "  '23/06/19 22:28:51 INFO Client: Setting up container launch context for our AM\\n',\n",
       "  '23/06/19 22:28:51 INFO Client: Setting up the launch environment for our AM container\\n',\n",
       "  '23/06/19 22:28:51 INFO Client: Preparing resources for our AM container\\n',\n",
       "  '23/06/19 22:28:51 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\\n',\n",
       "  '23/06/19 22:28:52 INFO Client: Uploading resource file:/tmp/spark-8e310cd9-1dbf-47ba-be27-52d437a1e514/__spark_libs__7312108380004635906.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687213661089_0004/__spark_libs__7312108380004635906.zip\\n',\n",
       "  '23/06/19 22:28:53 INFO Client: Uploading resource file:/data/master_volume/examples/spark.py -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687213661089_0004/spark.py\\n',\n",
       "  '23/06/19 22:28:53 INFO Client: Uploading resource file:/usr/local/spark/python/lib/pyspark.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687213661089_0004/pyspark.zip\\n',\n",
       "  '23/06/19 22:28:53 INFO Client: Uploading resource file:/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687213661089_0004/py4j-0.10.9.7-src.zip\\n',\n",
       "  '23/06/19 22:28:53 INFO Client: Uploading resource file:/tmp/spark-8e310cd9-1dbf-47ba-be27-52d437a1e514/__spark_conf__6454335244428709880.zip -> hdfs://namenode:9000/user/root/.sparkStaging/application_1687213661089_0004/__spark_conf__.zip\\n',\n",
       "  '23/06/19 22:28:53 INFO SecurityManager: Changing view acls to: root\\n',\n",
       "  '23/06/19 22:28:53 INFO SecurityManager: Changing modify acls to: root\\n',\n",
       "  '23/06/19 22:28:53 INFO SecurityManager: Changing view acls groups to: \\n',\n",
       "  '23/06/19 22:28:53 INFO SecurityManager: Changing modify acls groups to: \\n',\n",
       "  '23/06/19 22:28:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY\\n',\n",
       "  '23/06/19 22:28:53 INFO Client: Submitting application application_1687213661089_0004 to ResourceManager\\n',\n",
       "  '23/06/19 22:28:54 INFO YarnClientImpl: Submitted application application_1687213661089_0004\\n',\n",
       "  '23/06/19 22:28:55 INFO Client: Application report for application_1687213661089_0004 (state: ACCEPTED)\\n',\n",
       "  '23/06/19 22:28:55 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: AM container is launched, waiting for AM container to Register with RM\\n',\n",
       "  '\\t ApplicationMaster host: N/A\\n',\n",
       "  '\\t ApplicationMaster RPC port: -1\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687213733858\\n',\n",
       "  '\\t final status: UNDEFINED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687213661089_0004/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/19 22:28:56 INFO Client: Application report for application_1687213661089_0004 (state: ACCEPTED)\\n',\n",
       "  '23/06/19 22:28:57 INFO Client: Application report for application_1687213661089_0004 (state: ACCEPTED)\\n',\n",
       "  '23/06/19 22:28:58 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:28:58 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: N/A\\n',\n",
       "  '\\t ApplicationMaster host: 4c10c74424b8\\n',\n",
       "  '\\t ApplicationMaster RPC port: 43017\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687213733858\\n',\n",
       "  '\\t final status: UNDEFINED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687213661089_0004/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/19 22:28:59 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:00 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:01 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:02 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:03 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:04 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:05 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:06 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:07 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:08 INFO Client: Application report for application_1687213661089_0004 (state: RUNNING)\\n',\n",
       "  '23/06/19 22:29:09 INFO Client: Application report for application_1687213661089_0004 (state: FINISHED)\\n',\n",
       "  '23/06/19 22:29:09 INFO Client: \\n',\n",
       "  '\\t client token: N/A\\n',\n",
       "  '\\t diagnostics: N/A\\n',\n",
       "  '\\t ApplicationMaster host: 4c10c74424b8\\n',\n",
       "  '\\t ApplicationMaster RPC port: 43017\\n',\n",
       "  '\\t queue: default\\n',\n",
       "  '\\t start time: 1687213733858\\n',\n",
       "  '\\t final status: SUCCEEDED\\n',\n",
       "  '\\t tracking URL: http://resourcemanager:8088/proxy/application_1687213661089_0004/\\n',\n",
       "  '\\t user: root\\n',\n",
       "  '23/06/19 22:29:09 INFO ShutdownHookManager: Shutdown hook called\\n',\n",
       "  '23/06/19 22:29:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-8e310cd9-1dbf-47ba-be27-52d437a1e514\\n',\n",
       "  '23/06/19 22:29:09 INFO ShutdownHookManager: Deleting directory /tmp/spark-81ac8ce4-10b1-4899-bace-bb5bd2c5d11d\\n'])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_in_master(\"spark-submit --master yarn --deploy-mode cluster /data/master_volume/examples/spark.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16ee6e9c-7866-4753-8df4-48790e9e12dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "\n",
      "Micha≈Ç\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(\"/spark-result/dataframe-select/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75e37686-f169-49d2-9b8a-237bfba6c056",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name\n",
      "\n",
      "Micha≈Ç\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_hdfs_output(\"/spark-result/sql-select/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290c8970-73e2-4768-bef5-3b658b8ec6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
